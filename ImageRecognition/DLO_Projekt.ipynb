{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf513ef",
   "metadata": {},
   "source": [
    "# Classification of images\n",
    "\n",
    "Given are images that represent the hand gestures of the game \"paper, rock, scissors\" and that are stored in the respective\n",
    "folders. In an additionally folder random images (that do not correspond to one of these gestures) are stored.\n",
    "A classification model is to be build that classifies an image to one of the four labels \"paper\", \"rock\", \"scissors\" or \"rest\".\n",
    "\n",
    "This project was created by Julian Kartte. For a more detailed description check out my [github](https://github.com/juliankartte/showroom). In case you are interested in getting the date used in this project feel free to contact me on [linkedin](https://www.linkedin.com/in/julian-kartte-aa64a9237/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be16ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import os\n",
    "import csv\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import random\n",
    "import itertools\n",
    "import json\n",
    "from tensorflow.keras.constraints import max_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca07158",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905ade98",
   "metadata": {},
   "source": [
    "**List of included parameters**:\n",
    "- **x_scale**: Number of pixels on the x-axis\n",
    "- **y_scale**: Number of pixels on the y-axis\n",
    "- **validation_split**: Ration of data in validation set\n",
    "\n",
    "\n",
    "- **use_data_preprocessing**: Doubles the amount of input-data bei rotating them by 180 degrees\n",
    "- **use_data_augmentation**: Use data augmentation in keras ImageGenerators\n",
    "\n",
    "\n",
    "- **modelname**: select one of the following models\n",
    " - FarahAmalia (acc=96): https://medium.com/geekculture/rock-paper-scissors-image-classification-using-cnn-eefe4569b415\n",
    " - Devakumar (acc=90): https://www.kaggle.com/code/imdevskp/rock-paper-scissors-image-classification-using-cnn/notebook\n",
    " - Aditya (acc=100): https://www.kaggle.com/code/recursion17/rockpaperscissors-100-accuracy\n",
    " - Custom: custom model\n",
    " - Custom_constraints: custom model with constraints\n",
    " \n",
    " \n",
    "- **optimizer**: select one of the following optimizers\n",
    " - Adam\n",
    " - Adadelta\n",
    " - Adagrad\n",
    " - RMSprop\n",
    " - SGD\n",
    "\n",
    "\n",
    "- **loss_function**: sets the loss_function of the model\n",
    "- **epochs**: number of epochs\n",
    "- **batch_size**: sets the batch size\n",
    "- **dropout_ration**: sets the dropout_ratio of dropout_layers\n",
    "\n",
    "\n",
    "- **learning_rate_reduction**: Determines whether to use learning rate reduction on plateau or not.\n",
    "- **learning_rate_monitor**: monitored quantity for learning_rate_reduction\n",
    "- **learning_rate_patience**: number of epochs to wait\n",
    "- **learning_rate_verbose**: update messages\n",
    "- **learning_rate_factor**: ration of reduction\n",
    "- **learnint_rate_start_lr**: sets starting learning rate\n",
    "- **learning_rate_min_lr**: minimal learning rate\n",
    "\n",
    "\n",
    "- **early_stopping**: Use early stopping or not\n",
    "- **early_stopping_monitor**: monitored quantity\n",
    "- **early_stopping_min_delta**: minimal difference between quantity needed to be seen as a decrease\n",
    "- **early_stopping_patience**: number of rounds waiting since last decrease before stopping\n",
    "- **early_stopping_verbose**: output messages\n",
    "- **early_stopping_restore_best_weights**: restore best weights after early stopping\n",
    "\n",
    "\n",
    "- **use_kaggle_data**: uses provided data\n",
    "- **use_grayscaled_data**: uses provided data turned into grayscale\n",
    "- **use_grayscaled_augmented_data**: uses augmented data turned into grayscale\n",
    "- **use_rest_class**: uses rest class or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021bde93",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_config = {\n",
    "    'x_scale': [30], # 60, 120\n",
    "    'y_scale': [20], # 40, 80\n",
    "    'validation_split': [0.2],\n",
    "    \n",
    "    'use_data_augmentation': [True],\n",
    "    \n",
    "    'modelname': ['Custom_constraints'], #'Custom', 'Aditya', 'FarahAmalia', 'Devakumar'],\n",
    "    'optimizer': [\n",
    "        tf.keras.optimizers.Adam(),\n",
    "        #tf.keras.optimizers.Adadelta(),\n",
    "        #tf.keras.optimizers.Adagrad(),\n",
    "        #tf.keras.optimizers.Adamax(),\n",
    "        #tf.keras.optimizers.RMSprop(),\n",
    "        #tf.keras.optimizers.SGD()\n",
    "        ],\n",
    "    \n",
    "    'loss_function': ['categorical_crossentropy'],\n",
    "    'epochs': [50],\n",
    "    'batch_size': [16],\n",
    "    'dropout_ratio': [0.3],\n",
    "\n",
    "    'learning_rate_reduction': [True],\n",
    "    'learning_rate_monitor': ['val_categorical_accuracy'],\n",
    "    'learning_rate_patience': [2],\n",
    "    'learning_rate_verbose': [1],\n",
    "    'learning_rate_factor': [0.25],\n",
    "    'learning_rate_start_lr': [0.001],\n",
    "    'learning_rate_min_lr': [0.000003],\n",
    "\n",
    "    'early_stopping': [True],\n",
    "    'early_stopping_monitor': ['val_categorical_accuracy'], #'val_loss'],\n",
    "    'early_stopping_min_delta': [0],\n",
    "    'early_stopping_patience': [10],\n",
    "    'early_stopping_verbose': [1],\n",
    "    'early_stopping_restore_best_weights': [True],\n",
    "\n",
    "    'use_grayscaled_data': [False],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264d0d4c",
   "metadata": {},
   "source": [
    "# Data augmentation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14275b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(input_dict):\n",
    "    \"\"\"\n",
    "    Loads the images from the path './Traindata' according to the configuration in gs_config.\n",
    "    Returns train_generator, validation_generator.\n",
    "    \"\"\"\n",
    "    \n",
    "    x_scale: int = input_dict['x_scale']\n",
    "    y_scale: int = input_dict['y_scale']\n",
    "    batch_size: int = input_dict['batch_size']\n",
    "    data_augmentation: bool = input_dict['use_data_augmentation']\n",
    "    validation_split: float = input_dict['validation_split']\n",
    "    use_grayscaled_data : bool = input_dict['use_grayscaled_data']\n",
    "        \n",
    "    if use_grayscaled_data:\n",
    "        color_mode = 'grayscale'\n",
    "    else:\n",
    "        color_mode = 'rgb'\n",
    "\n",
    "    if data_augmentation :\n",
    "        datagen = ImageDataGenerator(rescale = 1.0/255,\n",
    "                                     rotation_range = 20,\n",
    "                                     width_shift_range = 0.2,\n",
    "                                     height_shift_range = 0.2,\n",
    "                                     shear_range = 0.2,\n",
    "                                     zoom_range = 0.2,\n",
    "                                     horizontal_flip = True,\n",
    "                                     fill_mode = 'nearest',\n",
    "                                     validation_split = validation_split\n",
    "                              )\n",
    "        valgen = ImageDataGenerator(rescale = 1.0/255,\n",
    "                                    validation_split = validation_split)\n",
    "    else:\n",
    "        datagen = ImageDataGenerator(rescale = 1.0/255,\n",
    "                                     validation_split = validation_split\n",
    "                                    )\n",
    "        valgen = ImageDataGenerator(rescale = 1.0/255,\n",
    "                                     validation_split = validation_split\n",
    "                                   )\n",
    "\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "            './Traindata',\n",
    "            target_size=(x_scale, y_scale),\n",
    "            color_mode = color_mode,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            class_mode='categorical',\n",
    "            subset='training',\n",
    "    )\n",
    "    \n",
    "    validation_generator = valgen.flow_from_directory(\n",
    "            './Traindata', \n",
    "            target_size=(x_scale, y_scale),\n",
    "            color_mode = color_mode,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            class_mode='categorical',\n",
    "            subset='validation')\n",
    "\n",
    "    return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2369b696",
   "metadata": {},
   "source": [
    "# Creating and fitting the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f1fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(optimizer, labels_count, input_dict):    \n",
    "    \"\"\"\n",
    "    Builds a model according to the specification in the current iteration input_dict of gs_config.\n",
    "    Returns the built model.\n",
    "    \"\"\"\n",
    "    \n",
    "    modelname = input_dict['modelname']\n",
    "    loss_function = input_dict['loss_function']\n",
    "    x_scale = input_dict['x_scale']\n",
    "    y_scale = input_dict['y_scale']\n",
    "    lr_start = input_dict['learning_rate_start_lr']\n",
    "    dropout_ratio = input_dict['dropout_ratio']\n",
    "    \n",
    "    if input_dict['use_grayscaled_data']:\n",
    "        color_layer = 1\n",
    "    else:\n",
    "        color_layer = 3\n",
    "    \n",
    "    if modelname == 'FarahAmalia':\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(64, (3,3), activation=tf.nn.relu, input_shape=(x_scale, y_scale, color_layer)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "            tf.keras.layers.Conv2D(64, (3,3), activation=tf.nn.relu, padding = 'Same'),\n",
    "            tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "            tf.keras.layers.Conv2D(128, (3,3), activation=tf.nn.relu, padding = 'Same'),\n",
    "            tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "            tf.keras.layers.Flatten(),\n",
    "\n",
    "            tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(labels_count, activation = tf.nn.softmax),\n",
    "        ])\n",
    "        \n",
    "    elif modelname == 'Devakumar':\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(x_scale, y_scale, color_layer)),\n",
    "            tf.keras.layers.MaxPooling2D(2,2),\n",
    "            tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(2,2),\n",
    "            tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(2,2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(labels_count, activation='sigmoid'),\n",
    "        ])\n",
    "        \n",
    "    elif modelname == 'Aditya':\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(64, (5,5), activation=tf.nn.relu, input_shape=(x_scale, y_scale, color_layer)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "            tf.keras.layers.Conv2D(64, (3,3), activation=tf.nn.relu, padding = 'Same'),\n",
    "            tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "            tf.keras.layers.Conv2D(128, (3,3), activation=tf.nn.relu, padding = 'Same'),\n",
    "            tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "            tf.keras.layers.Flatten(),\n",
    "\n",
    "            tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(labels_count, activation = tf.nn.softmax)\n",
    "        ])\n",
    "\n",
    "    elif modelname == 'Custom':\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(64, (5,5), activation=tf.nn.relu, input_shape=(x_scale, y_scale, color_layer)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "            tf.keras.layers.Conv2D(64, (3,3), activation=tf.nn.relu, padding = 'Same'),\n",
    "            tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "            tf.keras.layers.Conv2D(128, (3,3), activation=tf.nn.relu, padding = 'Same'),\n",
    "            tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "            tf.keras.layers.Flatten(),\n",
    "\n",
    "            tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(labels_count, activation = tf.nn.softmax)\n",
    "        ])\n",
    "        \n",
    "    elif modelname == 'Custom_constraints':\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(64, (3,3), activation=tf.nn.relu, kernel_constraint=max_norm(3), \n",
    "                                   input_shape=(x_scale, y_scale, color_layer)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(dropout_ratio),\n",
    "\n",
    "            tf.keras.layers.Conv2D(64, (3,3), activation=tf.nn.relu, padding = 'Same', kernel_constraint=max_norm(3)),\n",
    "            tf.keras.layers.MaxPooling2D(2,2),\n",
    "            tf.keras.layers.Dropout(dropout_ratio),\n",
    "\n",
    "            tf.keras.layers.Conv2D(128, (3,3), activation=tf.nn.relu, padding = 'Same', kernel_constraint=max_norm(3)),\n",
    "            tf.keras.layers.MaxPooling2D(2,2),\n",
    "            tf.keras.layers.Dropout(dropout_ratio),\n",
    "\n",
    "            tf.keras.layers.Flatten(),\n",
    "\n",
    "            tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(labels_count, activation = tf.nn.softmax),\n",
    "        ])\n",
    "    \n",
    "    temp_metrics = [\n",
    "        tf.keras.metrics.CategoricalAccuracy(name='categorical_accuracy'),\n",
    "        tf.keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    "\n",
    "    if type(optimizer) == type(tf.keras.optimizers.Adam()):\n",
    "        temp_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_start)\n",
    "    elif type(optimizer) == type(tf.keras.optimizers.Adamax()):\n",
    "        temp_optimizer = tf.keras.optimizers.Adamax(learning_rate=lr_start)\n",
    "    elif type(optimizer) == type(tf.keras.optimizers.Adagrad()):\n",
    "        temp_optimizer = tf.keras.optimizers.Adagrad(learning_rate=lr_start)\n",
    "    elif type(optimizer) == type(tf.keras.optimizers.Adadelta()):\n",
    "        temp_optimizer = tf.keras.optimizers.Adadelta(learning_rate=lr_start)\n",
    "    elif type(optimizer) == type(tf.keras.optimizers.Nadam()):\n",
    "        temp_optimizer = tf.keras.optimizers.Nadam(learning_rate=lr_start)\n",
    "    elif type(optimizer) == type(tf.keras.optimizers.RMSprop()):\n",
    "        temp_optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr_start)\n",
    "    elif type(optimizer) == type(tf.keras.optimizers.SGD()):\n",
    "        temp_optimizer = tf.keras.optimizers.SGD(learning_rate=lr_start)\n",
    "    else:\n",
    "        print('ERROR in get_model!')\n",
    "        return None\n",
    "    model.compile(loss=loss_function, optimizer=temp_optimizer, metrics=[temp_metrics], experimental_run_tf_function=False)\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_model(model, train_generator, validation_generator, iter_dict):\n",
    "    \"\"\"\n",
    "    Fits the model to the train_generator and validates on the validation_generator.\n",
    "    Returns the history of the model and the stopped epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    callback_list = None\n",
    "\n",
    "    if iter_dict['learning_rate_reduction']:\n",
    "        learning_rate_reduction = ReduceLROnPlateau(monitor=iter_dict['learning_rate_monitor'],\n",
    "                                            patience=iter_dict['learning_rate_patience'],\n",
    "                                            verbose=iter_dict['learning_rate_verbose'],\n",
    "                                            factor=iter_dict['learning_rate_factor'],\n",
    "                                            min_lr=iter_dict['learning_rate_min_lr'])\n",
    "        if callback_list == None:\n",
    "            callback_list = [learning_rate_reduction]\n",
    "        else:\n",
    "            callback_list.append(learning_rate_reduction)\n",
    "        \n",
    "    if iter_dict['early_stopping']:\n",
    "        early_stopping = EarlyStopping(monitor=iter_dict['early_stopping_monitor'],\n",
    "                                       min_delta=iter_dict['early_stopping_min_delta'], \n",
    "                                       patience=iter_dict['early_stopping_patience'], \n",
    "                                       verbose=iter_dict['early_stopping_verbose'], \n",
    "                                       restore_best_weights=iter_dict['early_stopping_restore_best_weights'])\n",
    "        if callback_list == None:\n",
    "            callback_list = [early_stopping]\n",
    "        else:\n",
    "            callback_list.append(early_stopping)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=iter_dict['epochs'],\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=callback_list)\n",
    "    \n",
    "    if iter_dict['early_stopping']:\n",
    "        return history, early_stopping\n",
    "    else:\n",
    "        return history, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca435d",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "\n",
    "- get_permutations creates all possible permutations of gs_config. Returns a list of tuples T. Every T consists of tuples t of size 2. The first element of t is the key of the dictionaries and the second element is the value of the dictionaries. See the following example:\n",
    "\n",
    "    `[(('modelname', 'FarahAmalia'),\n",
    "    ('optimizer', 'Adam'),\n",
    "    ('loss_function', 'categorical_crossentropy'),\n",
    "    ('metric', 'acc'),\n",
    "    ('epochs', 10),\n",
    "    ('batch_size', 32)), ...]`\n",
    "\n",
    "\n",
    "- Function grid_search iterates through all tuples T. In every iteration one we take on T and turn it into a dictionary, that gets returned at the end of the function. One dictionary of this kind includes all the parameters and the corresponding values for one run of grid_search. Example:\n",
    "\n",
    "    `{'modelname': 'FarahAmalia',\n",
    "     'optimizer': 'Adam',\n",
    "     'loss_function': 'categorical_crossentropy',\n",
    "     'metric': 'acc',\n",
    "     'epochs': 10,\n",
    "     'batch_size': 32}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_permutations(dictionary):\n",
    "    '''\n",
    "    Creates all permutations of the input dictionary. \n",
    "    Returns these iterations as a list of tupels of tupels.\n",
    "    '''\n",
    "    temp_dict = {}\n",
    "    \n",
    "    for key, value in dictionary.items():\n",
    "        temp_dict[key] = [(key, v) for v in value]\n",
    "    \n",
    "    elements = list(temp_dict.values())\n",
    "        \n",
    "    return list(itertools.product(*elements))\n",
    "\n",
    "def grid_search():\n",
    "    \"\"\"\n",
    "    For every iteration: get the data and the model corresponding to the settings in that iteration of gs_config.\n",
    "    Fits the model and returns:\n",
    "    - best_model_dict: dictionary of best models for different metrics\n",
    "    - model_result: results of every model\n",
    "    \"\"\"\n",
    "    \n",
    "    best_model_dict = {\n",
    "        'val_loss': None,\n",
    "        'val_categorical_accuracy': None,\n",
    "        'val_binary_accuracy': None,\n",
    "        'val_precision': None,\n",
    "        'val_recall': None\n",
    "    }\n",
    "        \n",
    "    permutations = get_permutations(gs_config)\n",
    "    labels = os.listdir('./Traindata')\n",
    "\n",
    "    model_results = []\n",
    "    for p in permutations:\n",
    "        iter_dict = dict(p)\n",
    "        print('\\nActual permutation:')\n",
    "        print(json.dumps(str(iter_dict), indent=1))\n",
    "        train_gen, val_gen = get_data(iter_dict)\n",
    "\n",
    "        default_optimizer = iter_dict['optimizer']\n",
    "        model = get_model(optimizer=default_optimizer, labels_count=len(labels), input_dict=iter_dict)\n",
    "        history, early_stopping = fit_model(model, train_gen, val_gen, iter_dict)\n",
    "\n",
    "        # save best models\n",
    "        for key in best_model_dict.keys():\n",
    "            if key == 'val_loss' and (best_model_dict[key] is None or min(history.history[key]) < min(best_model_dict[key].history.history[key])):\n",
    "                    best_model_dict[key] = model\n",
    "            elif best_model_dict[key] is None or max(history.history[key]) > max(best_model_dict[key].history.history[key]):\n",
    "                    best_model_dict[key] = model\n",
    "        model_results.append((iter_dict, history, early_stopping))\n",
    "       \n",
    "    return best_model_dict, model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13616fe9",
   "metadata": {},
   "source": [
    "# Running grid_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e13e6f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_models, results  = grid_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfcefd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb04424e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
